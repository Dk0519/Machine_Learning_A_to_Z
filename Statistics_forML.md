# ğŸ“Š Statistics for Machineâ€¯Learning

> *â€œStatistics is the grammar of science.â€* â€” Karl Pearson

---

## âœ¨ What **is** Statistics?

Statistics is the discipline that **collects, organizes, summarizes, analyzes, and draws conclusions** from data.  In the context of machineâ€‘learning (ML), statistics provides the theoretical backbone for **building models**, **quantifying uncertainty**, and **validating results**.

* **Population**: the complete set of items/events we care about.
* **Sample**: a (usually small) subset of the population used to make inferences.
* **Parameter** (Greek letters, e.g. Î¼, Ïƒ): a descriptive measure of the population (unknown!).
* **Statistic** (Latin letters, e.g. $\bar{x}$, *s*): a descriptive measure computed from a sample.

---

## ğŸ—‚ï¸ Types of Data

| Category                      | Subâ€‘types      | Description       | Examples                      | Typical ML Encoding |
| ----------------------------- | -------------- | ----------------- | ----------------------------- | ------------------- |
| **Qualitative (Categorical)** | **Nominal**    | Unordered labels  | browserÂ {Chrome, Safari}      | Oneâ€‘hot             |
|                               | **Ordinal**    | Ordered labels    | Likert scaleÂ {Poorâ†’Excellent} | Ordinal/Target      |
| **Quantitative (Numeric)**    | **Discrete**   | Integers / counts | clicks per session            | Asâ€‘is or log        |
|                               | **Continuous** | Any real number   | temperatureâ€¯Â°C                | Normalization       |

> ğŸ’¡ **Why it matters:** Choosing the wrong encoding can break distanceâ€‘based models (e.g., kâ€‘NN treats category codes as numeric distance).

<details>
<summary><strong>More on Encoding ğŸ› ï¸</strong></summary>

* **Label Encoding** â€“ preserves order â†’ good for ordinal.
* **Oneâ€‘Hot / Dummy** â€“ breaks a column into *k* binary flags â†’ default for nominal.
* **Frequency Encoding** â€“ map category to its empirical probability.
* **Target Encoding** â€“ replace category with mean target â†’ powerful but risk of leakage.

</details>

## ğŸŒ³ Two Main Branches

| Branch          | Goal                                              | Typical Questions                               |
| --------------- | ------------------------------------------------- | ----------------------------------------------- |
| **Descriptive** | Condense & visualize data you *have*              | *â€œWhat is the average clickâ€‘throughâ€‘rate?â€*     |
| **Inferential** | Generalize beyond the data & quantify uncertainty | *â€œWill the new UI raise CTR across all users?â€* |

Below, each branch is expandable. Click to dive in! â¤µï¸

---

<details>
<summary><strong>ğŸ“ˆ Descriptive Statistics</strong></summary>

### 1. Measures of Central Tendency (MCT) ğŸ§­

| Symbol      | Name                | Formula                                  | Derivation Sketch                                                            |        |    |
| ----------- | ------------------- | ---------------------------------------- | ---------------------------------------------------------------------------- | ------ | -- |
| $\bar{x}$   | **Mean**            | $\bar{x}=\dfrac{1}{n}\sum_{i=1}^{n}x_i$  | Minimize squared error $\sum (x_i-c)^2$ â‡’ set derivative to 0 â‡’ $c=\bar{x}$. |        |    |
| $\tilde{x}$ | **Median**          | Middle value (or average of two middles) | Minimizes absolute error (\sum                                               | x\_i-c | ). |
| *Mode*      | Most frequent value | N/A                                      | Useful for categorical features.                                             |        |    |

### 2. Measures of Dispersion (MD) ğŸ¯

| Symbol | Name                     | Formula                                    | Interpretation                                                     |
| ------ | ------------------------ | ------------------------------------------ | ------------------------------------------------------------------ |
| $s^2$  | **Sample Variance**      | $s^2 = \dfrac{1}{n-1}\sum (x_i-\bar{x})^2$ | Average squared deviation; divisor *(nâ€‘1)* is Besselâ€™s correction. |
| $s$    | **Std.Â Deviation**       | $s=\sqrt{s^2}$                             | Back to original units.                                            |
| IQR    | **Interâ€‘Quartile Range** | $Q_3-Q_1$                                  | Robust to outliers; great for box plots.                           |

> ğŸ’¡ **ML tieâ€‘in:** Feature scaling (zâ€‘score) uses mean & stdâ€‘dev; robust scaling uses median & IQR.

### 3. Shape of the Distribution ğŸŒ€

* **Skewness** $\gamma_1 = \dfrac{\mu_3}{\sigma^3}$  â€“ asymmetry.
* **Kurtosis** $\gamma_2 = \dfrac{\mu_4}{\sigma^4}-3$ â€“ tail heaviness.

### 4. Visual Tools ğŸ–¼ï¸

| Plot      | Best for                |
| --------- | ----------------------- |
| Histogram | Univariate distribution |
| Boxâ€‘plot  | Spread & outliers       |
| Pairâ€‘plot | Multivariate overview   |
| Heatâ€‘map  | Correlation matrix      |

### ğŸš€ Realâ€‘World ML Examples

* **EDA before modeling**: spot skewness â†’ apply logâ€‘transform.
* **Data quality checks**: high stdâ€‘dev in sensor readings may flag malfunction.

</details>

---

<details>
<summary><strong>ğŸ² Inferential Statistics</strong></summary>

### 1. Estimation ğŸ”

**Goal:** Use a sample to estimate population parameters.

| Type              | Output                                     | Formula / Method                      | ML Context                                     |                                                 |
| ----------------- | ------------------------------------------ | ------------------------------------- | ---------------------------------------------- | ----------------------------------------------- |
| **Point**         | Single value $\hat{\theta}$                | MLE: maximize (L(\theta)=\prod f(x\_i | \theta)).                                      | Fit model weights by MLE (e.g., LogisticÂ Reg.). |
| **Interval (CI)** | Range $[\hat{\theta}\pm z_{\alpha/2}\,SE]$ | $SE=\dfrac{s}{\sqrt{n}}$ for mean.    | Reporting Â±1.96Â·SE around validation accuracy. |                                                 |

### 2. Hypothesis Testing âš”ï¸

| Concept        | Symbol        | Typical Steps                                |
| -------------- | ------------- | -------------------------------------------- |
| Null vs Alt    | $H_0, H_1$    | State claims                                 |
| Testâ€‘statistic | *t, z, Ï‡Â², F* | Compute from data                            |
| pâ€‘value        | *p*           | Prob. of observing â‰¥ statistic if $H_0$ true |
| Decision       | Î±             | Reject if *p* < Î±                            |

> ğŸ§  **Key Idea:** Small *p* â†’ data is incompatible with $H_0$; doesnâ€™t *prove* ï»¿$H_1$.

**Common Tests**

| Test     | Useâ€‘case            | Assumptions      |
| -------- | ------------------- | ---------------- |
| *tâ€‘test* | Mean diff (n<30)    | Normality        |
| *zâ€‘test* | Mean diff (known Ïƒ) | Normality        |
| *Ï‡Â²*     | Categorical assoc.  | Expected freq â‰¥5 |
| *ANOVA*  | â‰¥3 group means      | Homoscedasticity |

### 3. Resampling & the CLT ğŸŒ€

* **Bootstrap**: Empirically approximate sampling distribution â†’ robust CIs.
* **Crossâ€‘validation**: Estimate generalization error.

### 4. Biasâ€‘Variance Tradeâ€‘off ğŸ¯

Derive expected test MSE:
$E[(y-\hat f(x))^2] = \underbrace{\text{Bias}^2}_{(E\hat f - f)^2} + \underbrace{\text{Variance}}_{E[(\hat f-E\hat f)^2]} + \sigma^2$

* **Highâ€‘bias models**: underfit (e.g., linear on nonâ€‘linear data).
* **Highâ€‘variance models**: overfit (deep tree without pruning).

### ğŸš€ Realâ€‘World ML Examples

* **A/B Testing**: Hypothesis test on conversion rate.
* **Early stopping**: Monitor CV error â†’ balance variance.
* **Ensembles**: Bagging (RandomÂ Forest) reduces variance via bootstrap.

</details>

---

## ğŸ“š Further Reading & Resources

1. *Pattern Recognition & Machine Learning* â€” C.Â Bishop, Ch.â€¯1â€‘2.
2. *An Introduction to Statistical Learning* â€” J.Â JamesÂ etÂ al., Ch.â€¯2â€‘5.
3. *Practical Statistics for Data Scientists* â€” B.Â Bruce.

---

> ğŸ **Next Steps:** Dive into probability distributions â¡ï¸ Normal, Bernoulli, Poisson, â€¦

---

Â©Â 2025Â PrashantÂ Yadav. Feel free to copy / share under CCâ€‘BYâ€‘SA 4.0.
